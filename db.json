{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"themes/Anisina/source/css/blog-style.css","path":"css/blog-style.css","modified":0,"renderable":1},{"_id":"themes/Anisina/source/css/syntax.styl","path":"css/syntax.styl","modified":0,"renderable":1},{"_id":"themes/Anisina/source/fonts/glyphicons-halflings-regular.eot","path":"fonts/glyphicons-halflings-regular.eot","modified":0,"renderable":1},{"_id":"themes/Anisina/source/fonts/glyphicons-halflings-regular.ttf","path":"fonts/glyphicons-halflings-regular.ttf","modified":0,"renderable":1},{"_id":"themes/Anisina/source/fonts/glyphicons-halflings-regular.woff","path":"fonts/glyphicons-halflings-regular.woff","modified":0,"renderable":1},{"_id":"themes/Anisina/source/fonts/glyphicons-halflings-regular.woff2","path":"fonts/glyphicons-halflings-regular.woff2","modified":0,"renderable":1},{"_id":"themes/Anisina/source/js/blog.js","path":"js/blog.js","modified":0,"renderable":1},{"_id":"themes/Anisina/source/js/bootstrap.min.js","path":"js/bootstrap.min.js","modified":0,"renderable":1},{"_id":"themes/Anisina/source/js/jquery.tagcloud.js","path":"js/jquery.tagcloud.js","modified":0,"renderable":1},{"_id":"themes/Anisina/source/js/totop.js","path":"js/totop.js","modified":0,"renderable":1},{"_id":"themes/Anisina/source/css/bootstrap.min.css","path":"css/bootstrap.min.css","modified":0,"renderable":1},{"_id":"themes/Anisina/source/fonts/glyphicons-halflings-regular.svg","path":"fonts/glyphicons-halflings-regular.svg","modified":0,"renderable":1},{"_id":"themes/Anisina/source/js/jquery.min.js","path":"js/jquery.min.js","modified":0,"renderable":1},{"_id":"themes/Anisina/source/js/jquery.js","path":"js/jquery.js","modified":0,"renderable":1}],"Cache":[{"_id":"themes/Anisina/LICENSE","hash":"2b209f06bebeb2a8c2b7e187e436f3e1e1fbc8a7","modified":1487124949739},{"_id":"themes/Anisina/README.md","hash":"412d514cbc7ba8705faaa614a322c63b48e91550","modified":1487124949739},{"_id":"themes/Anisina/_config.yml","hash":"1a6492e8ab3d6f3ab6a374d037dbff2b98a8eb22","modified":1487321135537},{"_id":"themes/Anisina/package.json","hash":"12541fbf56f785e4f5d486a55b4939f3033f625b","modified":1487124949743},{"_id":"source/_discarded/alentine-s-Day.md","hash":"7bddfe3e5ce1fefc950bde62b8ea589228fe3f84","modified":1487124949711},{"_id":"source/_posts/Dockerfile的创建.md","hash":"b6fcdffd0016739c4e22dca88c24406f5b7f7dd0","modified":1487322812276},{"_id":"source/_posts/Valentine-s-Day.md","hash":"515ce890daf908ba44f1074bf8a6e8b9d41a2d2c","modified":1487124949711},{"_id":"source/_posts/docker-本地Registry的部署.md","hash":"11e99456f17ed0fdc8e74036127d638b4e06dbe7","modified":1487321398835},{"_id":"source/_posts/机器学习-一-简单线性回归.md","hash":"ed33f6bd7dc373068026531d3d25f1f198299312","modified":1487743557284},{"_id":"source/tags/index.md","hash":"8fa1a71f1ed06b30a030a72c38196f529624fd40","modified":1487321135537},{"_id":"themes/Anisina/Screenshots/mobile-index.jpeg","hash":"cd75f77f5d865d42182e2233e354eeba9f114d98","modified":1487124949739},{"_id":"themes/Anisina/layout/404.ejs","hash":"1fe05722bd1b32bbe0ae4e3e880866f935e0ae11","modified":1487124949743},{"_id":"themes/Anisina/layout/index.ejs","hash":"a94ac678f6b24a46824d45ec058b0ab2105a92c9","modified":1487124949743},{"_id":"themes/Anisina/layout/layout.ejs","hash":"9d3d82a707b107f42db34ef5a8016693da2be742","modified":1487124949743},{"_id":"themes/Anisina/layout/page.ejs","hash":"95bbc74aa6d10cddddd7a5cd5d2a06482f5ea173","modified":1487321135537},{"_id":"themes/Anisina/layout/poetry.ejs","hash":"6c955d419050825e13d39c780d45aceafbf6552d","modified":1487124949743},{"_id":"themes/Anisina/layout/post.ejs","hash":"07a81a2b5c3cdb0692935e4a876219e5a5c645bd","modified":1487124949743},{"_id":"themes/Anisina/layout/tags.ejs","hash":"a144c3b5f4a70f881333e4de392a265469cdb649","modified":1487124949743},{"_id":"themes/Anisina/layout/works.ejs","hash":"1df954e54098cc4845295836374abed870789dcd","modified":1487124949743},{"_id":"themes/Anisina/languages_to_be_added/de.yml","hash":"424a9c1e6ab69334d7873f6574da02ca960aa572","modified":1487124949743},{"_id":"themes/Anisina/languages_to_be_added/default.yml","hash":"97326c9e6518d9f379778178b3b8f9a58434725d","modified":1487124949743},{"_id":"themes/Anisina/languages_to_be_added/en.yml","hash":"97326c9e6518d9f379778178b3b8f9a58434725d","modified":1487124949743},{"_id":"themes/Anisina/languages_to_be_added/es.yml","hash":"cb4eeca0ed3768a77e0cd216300f2b2549628b1b","modified":1487124949743},{"_id":"themes/Anisina/languages_to_be_added/no.yml","hash":"8ca475a3b4f8efe6603030f0013aae39668230e1","modified":1487124949743},{"_id":"themes/Anisina/languages_to_be_added/pl.yml","hash":"de7eb5850ae65ba7638e907c805fea90617a988c","modified":1487124949743},{"_id":"themes/Anisina/languages_to_be_added/ru.yml","hash":"42df7afeb7a35dc46d272b7f4fb880a9d9ebcaa5","modified":1487124949743},{"_id":"themes/Anisina/languages_to_be_added/zh-CN.yml","hash":"7bfcb0b8e97d7e5edcfca8ab26d55d9da2573c1c","modified":1487124949743},{"_id":"themes/Anisina/languages_to_be_added/zh-TW.yml","hash":"9acac6cc4f8002c3fa53ff69fb8cf66c915bd016","modified":1487124949743},{"_id":"source/_posts/Dockerfile的创建/docker.jpg","hash":"681e91e982bb01c245d328be255c206aac328a27","modified":1487322321851},{"_id":"source/_posts/docker-本地Registry的部署/docker.jpg","hash":"681e91e982bb01c245d328be255c206aac328a27","modified":1487321135537},{"_id":"source/_posts/机器学习-一-简单线性回归/ex1data1.txt","hash":"0c3804a5f2517fb2e1651cfdba74c347c27f3221","modified":1487658918437},{"_id":"source/_posts/机器学习-一-简单线性回归/figure1.png","hash":"c762f0f72ac5973f28a7f73a913faec8d745b325","modified":1487658918437},{"_id":"source/_posts/机器学习-一-简单线性回归/figure2.png","hash":"d8644f2737edf9197444e9b88c0197e4256e0042","modified":1487658918437},{"_id":"themes/Anisina/Screenshots/Anisina.png","hash":"146dd991f55a827a514259e20a51de1e9b07a13d","modified":1487124949739},{"_id":"themes/Anisina/layout/_partial/footer.ejs","hash":"3455a3cc578cfbdfc0c4340c768997c8a7f215ca","modified":1487321135537},{"_id":"themes/Anisina/layout/_partial/head.ejs","hash":"4e0d96cac503d4e3a5b254d8b8175c392971ce38","modified":1487124949743},{"_id":"themes/Anisina/layout/_partial/nav.ejs","hash":"3baa41d595e951efa1db34dd1789c6f8d3b094da","modified":1487124949743},{"_id":"themes/Anisina/layout/_partial/pagination.ejs","hash":"557d6bb069a1d48af49ae912994653f44b32a570","modified":1487124949743},{"_id":"themes/Anisina/source/css/blog-style.css","hash":"d064f7fd3b16dabca768a06e9779cbb7ba608ce7","modified":1487124949743},{"_id":"themes/Anisina/source/css/syntax.styl","hash":"f3f9ff0d1ebc4f7fa18d7e367b2ba2f0899adbd4","modified":1487124949743},{"_id":"themes/Anisina/source/fonts/glyphicons-halflings-regular.eot","hash":"86b6f62b7853e67d3e635f6512a5a5efc58ea3c3","modified":1487124949743},{"_id":"themes/Anisina/source/fonts/glyphicons-halflings-regular.ttf","hash":"44bc1850f570972267b169ae18f1cb06b611ffa2","modified":1487124949743},{"_id":"themes/Anisina/source/fonts/glyphicons-halflings-regular.woff","hash":"278e49a86e634da6f2a02f3b47dd9d2a8f26210f","modified":1487124949743},{"_id":"themes/Anisina/source/fonts/glyphicons-halflings-regular.woff2","hash":"ca35b697d99cae4d1b60f2d60fcd37771987eb07","modified":1487124949743},{"_id":"themes/Anisina/source/js/blog.js","hash":"0f805c744ef8a48c0abdd9d204cfc19ee6cafc14","modified":1487124949747},{"_id":"themes/Anisina/source/js/bootstrap.min.js","hash":"b3f2ef9f985e7906c9360756b73cd64bf7733647","modified":1487124949747},{"_id":"themes/Anisina/source/js/jquery.tagcloud.js","hash":"4e5fd0b07f3bd935f2e603710447e039e3677211","modified":1487124949747},{"_id":"themes/Anisina/source/js/totop.js","hash":"11ede60fccb7c763d6973f80efc78b47c0843746","modified":1487124949747},{"_id":"themes/Anisina/Screenshots/poetry-show.png","hash":"f5fdcd25026a87a0aafeebb1f19cdb3c0a81a666","modified":1487124949743},{"_id":"themes/Anisina/source/css/bootstrap.min.css","hash":"c5db932e115ff97af7b4512b947cde3ba2964db8","modified":1487124949743},{"_id":"themes/Anisina/source/fonts/glyphicons-halflings-regular.svg","hash":"de51a8494180a6db074af2dee2383f0a363c5b08","modified":1487124949743},{"_id":"themes/Anisina/source/js/jquery.min.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1487124949747},{"_id":"themes/Anisina/Screenshots/mobile-post.jpeg","hash":"2081cdff23a9a8c185a48d9aabcc9dc8e77833ec","modified":1487124949739},{"_id":"themes/Anisina/source/js/jquery.js","hash":"1852661bd11a09ca9b9cb63d1aa6ff390fffaf4e","modified":1487124949747},{"_id":"themes/Anisina/Screenshots/pc-index.png","hash":"b04094dac75cb656b4244c1dfaf246168a0f8926","modified":1487124949739},{"_id":"themes/Anisina/Screenshots/pc-post.png","hash":"cde56c0797b6ff8dd555fb1f8c3f9b21bceaa3be","modified":1487124949743},{"_id":"public/tags/index.html","hash":"ed7f56a9ee2f77d8abd65eb4a2fabc80a80b289b","modified":1487659193977},{"_id":"public/2017/02/17/docker-本地Registry的部署/index.html","hash":"93233cae436aa18979a00ad6f8225813bec120f3","modified":1487659193985},{"_id":"public/2017/02/14/Valentine-s-Day/index.html","hash":"b45bdfa2bd10d80fe97aa60ea630fee3a2582a43","modified":1487659193991},{"_id":"public/2017/02/17/Dockerfile的创建/index.html","hash":"6bea661db17bfc180862188620d5d1e7f2b1a178","modified":1487659193992},{"_id":"public/archives/index.html","hash":"a42e9e646210bc6da084aa093d84d8fe5241647d","modified":1487659193992},{"_id":"public/archives/2017/index.html","hash":"0e65128a1006a7dfcc0f9fb7a3630f5502442d62","modified":1487659193992},{"_id":"public/tags/docker/index.html","hash":"761a6a951527df1ecd16a3819fb252629bbda061","modified":1487659193992},{"_id":"public/archives/2017/02/index.html","hash":"49383313f2d6c6d9379cb17515f1aec4fee1cd33","modified":1487659193993},{"_id":"public/tags/Lover/index.html","hash":"5f733658d1464e517f5596bba067275d349a955e","modified":1487659193993},{"_id":"public/index.html","hash":"0da78ff65f96cb9803e2ea5830e591e62344b19f","modified":1487659193993},{"_id":"public/tags/智障/index.html","hash":"ba15d6c25175aac603a07d517a4b52a3dcdbc748","modified":1487659193993},{"_id":"public/tags/机器学习/index.html","hash":"a41cc3fcbec38e8437f1a2c31abecd24304c4b17","modified":1487659194004},{"_id":"public/tags/学习笔记/index.html","hash":"8095ef3ba399d88545556e39826d84b791ef556f","modified":1487659194004},{"_id":"public/2017/02/21/机器学习-一-简单线性回归/index.html","hash":"e6a5a1565c4e8447da27aec568e1dcfe22905f63","modified":1487742983123},{"_id":"public/fonts/glyphicons-halflings-regular.eot","hash":"86b6f62b7853e67d3e635f6512a5a5efc58ea3c3","modified":1487659194004},{"_id":"public/fonts/glyphicons-halflings-regular.woff","hash":"278e49a86e634da6f2a02f3b47dd9d2a8f26210f","modified":1487659194004},{"_id":"public/fonts/glyphicons-halflings-regular.woff2","hash":"ca35b697d99cae4d1b60f2d60fcd37771987eb07","modified":1487659194004},{"_id":"public/fonts/glyphicons-halflings-regular.ttf","hash":"44bc1850f570972267b169ae18f1cb06b611ffa2","modified":1487659194004},{"_id":"public/2017/02/17/docker-本地Registry的部署/docker.jpg","hash":"681e91e982bb01c245d328be255c206aac328a27","modified":1487659194277},{"_id":"public/2017/02/17/Dockerfile的创建/docker.jpg","hash":"681e91e982bb01c245d328be255c206aac328a27","modified":1487659194277},{"_id":"public/2017/02/21/机器学习-一-简单线性回归/ex1data1.txt","hash":"0c3804a5f2517fb2e1651cfdba74c347c27f3221","modified":1487659194278},{"_id":"public/2017/02/21/机器学习-一-简单线性回归/figure1.png","hash":"c762f0f72ac5973f28a7f73a913faec8d745b325","modified":1487659194278},{"_id":"public/2017/02/21/机器学习-一-简单线性回归/figure2.png","hash":"d8644f2737edf9197444e9b88c0197e4256e0042","modified":1487659194278},{"_id":"public/css/syntax.css","hash":"4616879fec214c9cc4f5835615348f0bbeabf2a9","modified":1487659194280},{"_id":"public/css/blog-style.css","hash":"d064f7fd3b16dabca768a06e9779cbb7ba608ce7","modified":1487659194280},{"_id":"public/fonts/glyphicons-halflings-regular.svg","hash":"de51a8494180a6db074af2dee2383f0a363c5b08","modified":1487659194283},{"_id":"public/js/blog.js","hash":"0f805c744ef8a48c0abdd9d204cfc19ee6cafc14","modified":1487659194285},{"_id":"public/js/jquery.tagcloud.js","hash":"4e5fd0b07f3bd935f2e603710447e039e3677211","modified":1487659194285},{"_id":"public/js/totop.js","hash":"11ede60fccb7c763d6973f80efc78b47c0843746","modified":1487659194285},{"_id":"public/js/bootstrap.min.js","hash":"b3f2ef9f985e7906c9360756b73cd64bf7733647","modified":1487659194285},{"_id":"public/js/jquery.min.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1487659194285},{"_id":"public/css/bootstrap.min.css","hash":"c5db932e115ff97af7b4512b947cde3ba2964db8","modified":1487659194285},{"_id":"public/js/jquery.js","hash":"1852661bd11a09ca9b9cb63d1aa6ff390fffaf4e","modified":1487659194285},{"_id":"source/_posts/机器学习-一-简单线性回归/figure3.png","hash":"0fddd9aae082d241704e0cde476829949df73650","modified":1487660076255},{"_id":"public/2017/02/21/机器学习-一-简单线性回归/figure3.png","hash":"0fddd9aae082d241704e0cde476829949df73650","modified":1487660119139},{"_id":"source/_posts/机器学习-一-简单线性回归/formula1.png","hash":"3723e9bf5fb97f9e4677dcd3b3d1bbfd192c4af0","modified":1487733552192},{"_id":"source/_posts/机器学习-一-简单线性回归/formula2.png","hash":"97d59bf0ab811eb85281ce9846c460be5bb89685","modified":1487734003855},{"_id":"source/_posts/机器学习-一-简单线性回归/formula3.png","hash":"6e602dec70a264e28489071b223e4534a85834a1","modified":1487738604582},{"_id":"source/_posts/机器学习-一-简单线性回归/formula4.png","hash":"c3ec653220f7e4f7ac1806bcb9620a5569f2d9a2","modified":1487741314408},{"_id":"public/2017/02/21/机器学习-一-简单线性回归/formula1.png","hash":"3723e9bf5fb97f9e4677dcd3b3d1bbfd192c4af0","modified":1487742635899},{"_id":"public/2017/02/21/机器学习-一-简单线性回归/formula2.png","hash":"97d59bf0ab811eb85281ce9846c460be5bb89685","modified":1487742635899},{"_id":"public/2017/02/21/机器学习-一-简单线性回归/formula4.png","hash":"c3ec653220f7e4f7ac1806bcb9620a5569f2d9a2","modified":1487742635899},{"_id":"public/2017/02/21/机器学习-一-简单线性回归/formula3.png","hash":"6e602dec70a264e28489071b223e4534a85834a1","modified":1487742635900}],"Category":[],"Data":[],"Page":[{"title":"tags","date":"2017-02-16T16:02:29.000Z","layout":"tags","_content":"","source":"tags/index.md","raw":"---\ntitle: tags\ndate: 2017-02-17 00:02:29\nlayout: tags\n---\n","updated":"2017-02-17T08:45:35.537Z","path":"tags/index.html","comments":1,"_id":"cizf5vg8900010tm838nkubte","content":"","excerpt":"","more":""}],"Post":[{"title":"Dockerfile的创建","date":"2017-02-17T08:52:27.000Z","author":"月牙天冲","header-img":"http://images2015.cnblogs.com/blog/1003956/201609/1003956-20160929094610156-2054520507.png","cdn":"header-off","_content":"\n\n* FROM <image name>\n\n指定容器基于哪个基础镜像创建\n\n\n* MAINTAINER <author name>\n\n设置镜像的作者\n\n\n* RUN <commend>\n\n在容器中运行的命令\n\n\n* ADD <src> <destination>\n\n复制文件指令, destination是容器内的路径。source可以是URL或者是启动配置上下文中的一个文件\n\n\n* CMD\n\n提供了容器默认的执行命令。 Dockerfile只允许使用一次CMD指令。 使用多个CMD会抵消之前所有的指令，只有最后一个指令生效,CMD有三种形式：\n```\nCMD [\"executable\",\"param1\",\"param2\"]\nCMD [\"param1\",\"param2\"]\nCMD command param1 param2\n```\n\n\n* EXPOSE <port>\n\n指定容器运行在监听的端口\n\n\n* ENTRYPOINT\n\n配置给容器一个可执行的命令，这意味着在每次使用镜像创建容器时一个特定的应用程序可以被设置为默认程序。同时也意味着该镜像每次被调用时仅能运行指定的应用。类似于CMD，Docker只允许一个ENTRYPOINT，多个ENTRYPOINT会抵消之前所有的指令，只执行最后的ENTRYPOINT指令。语法如下：\n```\nENTRYPOINT [\"executable\", \"param1\",\"param2\"]\nENTRYPOINT command param1 param2\n```\n\n\n* WORKDIR\n\n指定RUN、CMD与ENTRYPOINT命令的工作目录。语法如下：\n\n```\nWORKDIR /path/to/workdir\n```\n\n\n* ENV\n\n设置环境变量。它们使用键值对，增加运行程序的灵活性。语法如下：\n\n```\nENV <key> <value>\n```\n\n\n* USER\n镜像正在运行时设置一个UID。语法如下：\n\n```\nUSER <uid>\n```\n\n\n* VOLUME\n\n授权访问从容器内到主机上的目录。语法如下：\n\n```\nVOLUME [\"/data\"]\n```\n\n例子:\n```\nFROM localhost:5000/ubuntu:14.4\nMAINTAINER wudizhangzhi\nADD localfile.txt /home/targetfile.txt\nWORKDIR /home/\nRUN pip install -r targetfile.txt\nEXPOSE 7777\n```\n```bash\ndocker build -t wudizhangzhi/ubuntu:latest .\n```\n","source":"_posts/Dockerfile的创建.md","raw":"---\ntitle: Dockerfile的创建\ndate: 2017-02-17 16:52:27\nauthor: \"月牙天冲\"\nheader-img: \"http://images2015.cnblogs.com/blog/1003956/201609/1003956-20160929094610156-2054520507.png\"\ncdn: 'header-off'\ntags:\n  - docker\n---\n\n\n* FROM <image name>\n\n指定容器基于哪个基础镜像创建\n\n\n* MAINTAINER <author name>\n\n设置镜像的作者\n\n\n* RUN <commend>\n\n在容器中运行的命令\n\n\n* ADD <src> <destination>\n\n复制文件指令, destination是容器内的路径。source可以是URL或者是启动配置上下文中的一个文件\n\n\n* CMD\n\n提供了容器默认的执行命令。 Dockerfile只允许使用一次CMD指令。 使用多个CMD会抵消之前所有的指令，只有最后一个指令生效,CMD有三种形式：\n```\nCMD [\"executable\",\"param1\",\"param2\"]\nCMD [\"param1\",\"param2\"]\nCMD command param1 param2\n```\n\n\n* EXPOSE <port>\n\n指定容器运行在监听的端口\n\n\n* ENTRYPOINT\n\n配置给容器一个可执行的命令，这意味着在每次使用镜像创建容器时一个特定的应用程序可以被设置为默认程序。同时也意味着该镜像每次被调用时仅能运行指定的应用。类似于CMD，Docker只允许一个ENTRYPOINT，多个ENTRYPOINT会抵消之前所有的指令，只执行最后的ENTRYPOINT指令。语法如下：\n```\nENTRYPOINT [\"executable\", \"param1\",\"param2\"]\nENTRYPOINT command param1 param2\n```\n\n\n* WORKDIR\n\n指定RUN、CMD与ENTRYPOINT命令的工作目录。语法如下：\n\n```\nWORKDIR /path/to/workdir\n```\n\n\n* ENV\n\n设置环境变量。它们使用键值对，增加运行程序的灵活性。语法如下：\n\n```\nENV <key> <value>\n```\n\n\n* USER\n镜像正在运行时设置一个UID。语法如下：\n\n```\nUSER <uid>\n```\n\n\n* VOLUME\n\n授权访问从容器内到主机上的目录。语法如下：\n\n```\nVOLUME [\"/data\"]\n```\n\n例子:\n```\nFROM localhost:5000/ubuntu:14.4\nMAINTAINER wudizhangzhi\nADD localfile.txt /home/targetfile.txt\nWORKDIR /home/\nRUN pip install -r targetfile.txt\nEXPOSE 7777\n```\n```bash\ndocker build -t wudizhangzhi/ubuntu:latest .\n```\n","slug":"Dockerfile的创建","published":1,"updated":"2017-02-17T09:13:32.276Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cizf5vg8400000tm8t0vk05co","content":"<ul>\n<li>FROM <image name=\"\"></image></li>\n</ul>\n<p>指定容器基于哪个基础镜像创建</p>\n<ul>\n<li>MAINTAINER <author name=\"\"></author></li>\n</ul>\n<p>设置镜像的作者</p>\n<ul>\n<li>RUN <commend></commend></li>\n</ul>\n<p>在容器中运行的命令</p>\n<ul>\n<li>ADD <src> <destination></destination></src></li>\n</ul>\n<p>复制文件指令, destination是容器内的路径。source可以是URL或者是启动配置上下文中的一个文件</p>\n<ul>\n<li>CMD</li>\n</ul>\n<p>提供了容器默认的执行命令。 Dockerfile只允许使用一次CMD指令。 使用多个CMD会抵消之前所有的指令，只有最后一个指令生效,CMD有三种形式：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">CMD [&quot;executable&quot;,&quot;param1&quot;,&quot;param2&quot;]</div><div class=\"line\">CMD [&quot;param1&quot;,&quot;param2&quot;]</div><div class=\"line\">CMD command param1 param2</div></pre></td></tr></table></figure></p>\n<ul>\n<li>EXPOSE <port></port></li>\n</ul>\n<p>指定容器运行在监听的端口</p>\n<ul>\n<li>ENTRYPOINT</li>\n</ul>\n<p>配置给容器一个可执行的命令，这意味着在每次使用镜像创建容器时一个特定的应用程序可以被设置为默认程序。同时也意味着该镜像每次被调用时仅能运行指定的应用。类似于CMD，Docker只允许一个ENTRYPOINT，多个ENTRYPOINT会抵消之前所有的指令，只执行最后的ENTRYPOINT指令。语法如下：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">ENTRYPOINT [&quot;executable&quot;, &quot;param1&quot;,&quot;param2&quot;]</div><div class=\"line\">ENTRYPOINT command param1 param2</div></pre></td></tr></table></figure></p>\n<ul>\n<li>WORKDIR</li>\n</ul>\n<p>指定RUN、CMD与ENTRYPOINT命令的工作目录。语法如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">WORKDIR /path/to/workdir</div></pre></td></tr></table></figure>\n<ul>\n<li>ENV</li>\n</ul>\n<p>设置环境变量。它们使用键值对，增加运行程序的灵活性。语法如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">ENV &lt;key&gt; &lt;value&gt;</div></pre></td></tr></table></figure>\n<ul>\n<li>USER<br>镜像正在运行时设置一个UID。语法如下：</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">USER &lt;uid&gt;</div></pre></td></tr></table></figure>\n<ul>\n<li>VOLUME</li>\n</ul>\n<p>授权访问从容器内到主机上的目录。语法如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">VOLUME [&quot;/data&quot;]</div></pre></td></tr></table></figure>\n<p>例子:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">FROM localhost:5000/ubuntu:14.4</div><div class=\"line\">MAINTAINER wudizhangzhi</div><div class=\"line\">ADD localfile.txt /home/targetfile.txt</div><div class=\"line\">WORKDIR /home/</div><div class=\"line\">RUN pip install -r targetfile.txt</div><div class=\"line\">EXPOSE 7777</div></pre></td></tr></table></figure></p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><div class=\"line\">docker build -t wudizhangzhi/ubuntu:latest .</div></pre></td></tr></table></figure>\n","excerpt":"","more":"<ul>\n<li>FROM <image name=\"\"></image></li>\n</ul>\n<p>指定容器基于哪个基础镜像创建</p>\n<ul>\n<li>MAINTAINER <author name=\"\"></author></li>\n</ul>\n<p>设置镜像的作者</p>\n<ul>\n<li>RUN <commend></commend></li>\n</ul>\n<p>在容器中运行的命令</p>\n<ul>\n<li>ADD <src> <destination></destination></src></li>\n</ul>\n<p>复制文件指令, destination是容器内的路径。source可以是URL或者是启动配置上下文中的一个文件</p>\n<ul>\n<li>CMD</li>\n</ul>\n<p>提供了容器默认的执行命令。 Dockerfile只允许使用一次CMD指令。 使用多个CMD会抵消之前所有的指令，只有最后一个指令生效,CMD有三种形式：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">CMD [&quot;executable&quot;,&quot;param1&quot;,&quot;param2&quot;]</div><div class=\"line\">CMD [&quot;param1&quot;,&quot;param2&quot;]</div><div class=\"line\">CMD command param1 param2</div></pre></td></tr></table></figure></p>\n<ul>\n<li>EXPOSE <port></port></li>\n</ul>\n<p>指定容器运行在监听的端口</p>\n<ul>\n<li>ENTRYPOINT</li>\n</ul>\n<p>配置给容器一个可执行的命令，这意味着在每次使用镜像创建容器时一个特定的应用程序可以被设置为默认程序。同时也意味着该镜像每次被调用时仅能运行指定的应用。类似于CMD，Docker只允许一个ENTRYPOINT，多个ENTRYPOINT会抵消之前所有的指令，只执行最后的ENTRYPOINT指令。语法如下：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">ENTRYPOINT [&quot;executable&quot;, &quot;param1&quot;,&quot;param2&quot;]</div><div class=\"line\">ENTRYPOINT command param1 param2</div></pre></td></tr></table></figure></p>\n<ul>\n<li>WORKDIR</li>\n</ul>\n<p>指定RUN、CMD与ENTRYPOINT命令的工作目录。语法如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">WORKDIR /path/to/workdir</div></pre></td></tr></table></figure>\n<ul>\n<li>ENV</li>\n</ul>\n<p>设置环境变量。它们使用键值对，增加运行程序的灵活性。语法如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">ENV &lt;key&gt; &lt;value&gt;</div></pre></td></tr></table></figure>\n<ul>\n<li>USER<br>镜像正在运行时设置一个UID。语法如下：</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">USER &lt;uid&gt;</div></pre></td></tr></table></figure>\n<ul>\n<li>VOLUME</li>\n</ul>\n<p>授权访问从容器内到主机上的目录。语法如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">VOLUME [&quot;/data&quot;]</div></pre></td></tr></table></figure>\n<p>例子:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">FROM localhost:5000/ubuntu:14.4</div><div class=\"line\">MAINTAINER wudizhangzhi</div><div class=\"line\">ADD localfile.txt /home/targetfile.txt</div><div class=\"line\">WORKDIR /home/</div><div class=\"line\">RUN pip install -r targetfile.txt</div><div class=\"line\">EXPOSE 7777</div></pre></td></tr></table></figure></p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><div class=\"line\">docker build -t wudizhangzhi/ubuntu:latest .</div></pre></td></tr></table></figure>\n"},{"layout":"post","title":"Valentine's Day","subtitle":"for yumengqian","date":"2017-02-14T05:00:00.000Z","author":"月牙天冲","header-img":"http://olch8050n.bkt.clouddn.com/bac2.jpeg","cdn":"header-off","_content":"\n\nValentine's Day\n===\n\n\n![lover](http://olch8050n.bkt.clouddn.com/bac2.jpeg)\n* 用此文纪念和小于的第一个情人节\n","source":"_posts/Valentine-s-Day.md","raw":"---\nlayout: post\ntitle: Valentine's Day\nsubtitle: \"for yumengqian\"\ndate: 2017-02-14 13:00\nauthor: \"月牙天冲\"\nheader-img: \"http://olch8050n.bkt.clouddn.com/bac2.jpeg\"\ncdn: 'header-off'\ntags:\n\t- Lover\n\t- 智障\n---\n\n\nValentine's Day\n===\n\n\n![lover](http://olch8050n.bkt.clouddn.com/bac2.jpeg)\n* 用此文纪念和小于的第一个情人节\n","slug":"Valentine-s-Day","published":1,"updated":"2017-02-15T02:15:49.711Z","comments":1,"photos":[],"link":"","_id":"cizf5vg8b00020tm8103bez5d","content":"<h1 id=\"Valentine’s-Day\"><a href=\"#Valentine’s-Day\" class=\"headerlink\" title=\"Valentine’s Day\"></a>Valentine’s Day</h1><p><img src=\"http://olch8050n.bkt.clouddn.com/bac2.jpeg\" alt=\"lover\"></p>\n<ul>\n<li>用此文纪念和小于的第一个情人节</li>\n</ul>\n","excerpt":"","more":"<h1 id=\"Valentine’s-Day\"><a href=\"#Valentine’s-Day\" class=\"headerlink\" title=\"Valentine’s Day\"></a>Valentine’s Day</h1><p><img src=\"http://olch8050n.bkt.clouddn.com/bac2.jpeg\" alt=\"lover\"></p>\n<ul>\n<li>用此文纪念和小于的第一个情人节</li>\n</ul>\n"},{"title":"docker 本地Registry的部署","date":"2017-02-16T16:41:36.000Z","author":"月牙天冲","header-img":"docker.jpg","cdn":"header-off","_content":"\n# 1.本地Registry的部署\n运行下面命令获取registry镜像\n\n```bash\nsudo docker pull registry:2.1.1 # tag版本号随意设置\n```\n\n然后启动一个容器\n```bash\nsudo docker run -d -v /opt/registry:/var/lib/registry -p 5000:5000 --restart=always --name registry registry:2.1.1\n```\n\nRegistry服务默认会将上传的镜像保存在容器的/var/lib/registry，我们将主机的/opt/registry目录挂载到该目录，即可实现将镜像保存到主机的/opt/registry目录了。\n\n运行docker ps 查看容易运行情况\n```bash\ndocker ps\n```\n\n启动了registry服务，打开浏览器输入http://127.0.0.1:5000/v2 ，出现下面情况说明registry运行正常\n```bash\ncurl localhost:5000/v2\n```\n返回{}\n\n# 2.验证\n我的机器上有个hello-world的镜像，我们要通过docker tag将该镜像标志为要推送到私有仓库，\n\n```bash\nsudo docker tag hello-world:1.0.0 127.0.0.1:5000/hello-world:1.0.0\n\nsudo docker push 127.0.0.1:5000/hello-world:1.0.0\n```\n验证\n```bash\ncurl http://127.0.0.1:5000/v2/_catalog\n```\n返回json\n\n\n![logo](docker-本地Registry的部署/docker.jpg)\n","source":"_posts/docker-本地Registry的部署.md","raw":"---\ntitle: docker 本地Registry的部署\ndate: 2017-02-17 00:41:36\nauthor: \"月牙天冲\"\nheader-img: \"docker.jpg\"\ncdn: 'header-off'\ntags:\n    - docker\n---\n\n# 1.本地Registry的部署\n运行下面命令获取registry镜像\n\n```bash\nsudo docker pull registry:2.1.1 # tag版本号随意设置\n```\n\n然后启动一个容器\n```bash\nsudo docker run -d -v /opt/registry:/var/lib/registry -p 5000:5000 --restart=always --name registry registry:2.1.1\n```\n\nRegistry服务默认会将上传的镜像保存在容器的/var/lib/registry，我们将主机的/opt/registry目录挂载到该目录，即可实现将镜像保存到主机的/opt/registry目录了。\n\n运行docker ps 查看容易运行情况\n```bash\ndocker ps\n```\n\n启动了registry服务，打开浏览器输入http://127.0.0.1:5000/v2 ，出现下面情况说明registry运行正常\n```bash\ncurl localhost:5000/v2\n```\n返回{}\n\n# 2.验证\n我的机器上有个hello-world的镜像，我们要通过docker tag将该镜像标志为要推送到私有仓库，\n\n```bash\nsudo docker tag hello-world:1.0.0 127.0.0.1:5000/hello-world:1.0.0\n\nsudo docker push 127.0.0.1:5000/hello-world:1.0.0\n```\n验证\n```bash\ncurl http://127.0.0.1:5000/v2/_catalog\n```\n返回json\n\n\n![logo](docker-本地Registry的部署/docker.jpg)\n","slug":"docker-本地Registry的部署","published":1,"updated":"2017-02-17T08:49:58.835Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cizf5vg8g00040tm8zihq1x8w","content":"<h1 id=\"1-本地Registry的部署\"><a href=\"#1-本地Registry的部署\" class=\"headerlink\" title=\"1.本地Registry的部署\"></a>1.本地Registry的部署</h1><p>运行下面命令获取registry镜像</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><div class=\"line\">sudo docker pull registry:2.1.1 <span class=\"comment\"># tag版本号随意设置</span></div></pre></td></tr></table></figure>\n<p>然后启动一个容器<br><figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><div class=\"line\">sudo docker run <span class=\"_\">-d</span> -v /opt/registry:/var/lib/registry -p 5000:5000 --restart=always --name registry registry:2.1.1</div></pre></td></tr></table></figure></p>\n<p>Registry服务默认会将上传的镜像保存在容器的/var/lib/registry，我们将主机的/opt/registry目录挂载到该目录，即可实现将镜像保存到主机的/opt/registry目录了。</p>\n<p>运行docker ps 查看容易运行情况<br><figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><div class=\"line\">docker ps</div></pre></td></tr></table></figure></p>\n<p>启动了registry服务，打开浏览器输入<a href=\"http://127.0.0.1:5000/v2\" target=\"_blank\" rel=\"external\">http://127.0.0.1:5000/v2</a> ，出现下面情况说明registry运行正常<br><figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><div class=\"line\">curl localhost:5000/v2</div></pre></td></tr></table></figure></p>\n<p>返回{}</p>\n<h1 id=\"2-验证\"><a href=\"#2-验证\" class=\"headerlink\" title=\"2.验证\"></a>2.验证</h1><p>我的机器上有个hello-world的镜像，我们要通过docker tag将该镜像标志为要推送到私有仓库，</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><div class=\"line\">sudo docker tag hello-world:1.0.0 127.0.0.1:5000/hello-world:1.0.0</div><div class=\"line\"></div><div class=\"line\">sudo docker push 127.0.0.1:5000/hello-world:1.0.0</div></pre></td></tr></table></figure>\n<p>验证<br><figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><div class=\"line\">curl http://127.0.0.1:5000/v2/_catalog</div></pre></td></tr></table></figure></p>\n<p>返回json</p>\n<p><img src=\"/2017/02/17/docker-本地Registry的部署/docker.jpg\" alt=\"logo\"></p>\n","excerpt":"","more":"<h1 id=\"1-本地Registry的部署\"><a href=\"#1-本地Registry的部署\" class=\"headerlink\" title=\"1.本地Registry的部署\"></a>1.本地Registry的部署</h1><p>运行下面命令获取registry镜像</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><div class=\"line\">sudo docker pull registry:2.1.1 <span class=\"comment\"># tag版本号随意设置</span></div></pre></td></tr></table></figure>\n<p>然后启动一个容器<br><figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><div class=\"line\">sudo docker run <span class=\"_\">-d</span> -v /opt/registry:/var/lib/registry -p 5000:5000 --restart=always --name registry registry:2.1.1</div></pre></td></tr></table></figure></p>\n<p>Registry服务默认会将上传的镜像保存在容器的/var/lib/registry，我们将主机的/opt/registry目录挂载到该目录，即可实现将镜像保存到主机的/opt/registry目录了。</p>\n<p>运行docker ps 查看容易运行情况<br><figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><div class=\"line\">docker ps</div></pre></td></tr></table></figure></p>\n<p>启动了registry服务，打开浏览器输入<a href=\"http://127.0.0.1:5000/v2\">http://127.0.0.1:5000/v2</a> ，出现下面情况说明registry运行正常<br><figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><div class=\"line\">curl localhost:5000/v2</div></pre></td></tr></table></figure></p>\n<p>返回{}</p>\n<h1 id=\"2-验证\"><a href=\"#2-验证\" class=\"headerlink\" title=\"2.验证\"></a>2.验证</h1><p>我的机器上有个hello-world的镜像，我们要通过docker tag将该镜像标志为要推送到私有仓库，</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><div class=\"line\">sudo docker tag hello-world:1.0.0 127.0.0.1:5000/hello-world:1.0.0</div><div class=\"line\"></div><div class=\"line\">sudo docker push 127.0.0.1:5000/hello-world:1.0.0</div></pre></td></tr></table></figure>\n<p>验证<br><figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><div class=\"line\">curl http://127.0.0.1:5000/v2/_catalog</div></pre></td></tr></table></figure></p>\n<p>返回json</p>\n<p><img src=\"/2017/02/17/docker-本地Registry的部署/docker.jpg\" alt=\"logo\"></p>\n"},{"title":"机器学习(一):简单线性回归","date":"2017-02-21T02:31:06.000Z","author":"月牙天冲","_content":"\n## 1. 获取数据\n假设你是一家连锁店的老板，希望在另一个城市开分店，现在有一组城市的人口与利润的关系图，\n通过这些数据预测不同人口的城市的利润\n数据可以通过这个连接[下载](ex1data1.txt)\n\n\n```python\n# 导入模块\nimport numpy as np\nimport pandas as pd\nimport matlib.pyplot as plt\n\n# 读取数据\ndata = pd.read_csv('ex1data1.txt', names=['Population','Profit'], header=None) #设置列名\n# 画出数据的散点图查看\ndata.plot(kind='scatter', x='Population', y='Profit', figsize=(12,8))\nplt.show()\n```\n\n如图:  \n![example1](机器学习-一-简单线性回归/figure1.png)\n\n## 2.梯度下降\n\n>梯度下降:\n>假设 y = f(x), 求y最小值时候的x.\n>则 梯度 Δ = f'(x)\n>设定好一定的步长 λ\n>x <-- x - λ*Δ\n>代理初始的x后每次迭代x都往梯度下降的方向挪动一定的步长(其中Δ控制方向)。\n>迭代多次后,当两次x带入后获取的y数值相差极小时,此时的x为最优解\n\n\n## 3.实现简单的线性回归\n只有一个因素影响数据, 假设直线 Y = theta0 + theta1*X1  \n转换为向量的思想，则:Y = theta * (1, X)\n![formula](机器学习-一-简单线性回归/formula4.png)\n\n误差计算方法:\n![formula](机器学习-一-简单线性回归/formula1.png)\n\n即:\n![formula](机器学习-一-简单线性回归/formula2.png)\n\n```Python\ndef computeCast(X, Y, theta):\n  \"\"\"\n  计算误差的方法\n  X: 输入的数据,格式：[[1,2],[1,3],[1,4]...]\n  Y: 实际结果, 格式：[[1],[2],[3]...]\n  theta: 参数, 向量，格式：[0,0]\n  \"\"\"\n  error = np.sqrt(np.power(((X*theta.T) - Y), 2))\n  return np.sum(error)/(len(X)*2) #TODO 为什么要多除一个2？\n\n# 例\ntheta  = np.matrix(np.array([0,0]))\ncost = computeCast(X, Y, theta)\nprint cost # 32.0727338775\n\n```\n\n\n\n使用梯度下降的方法调节参数theta,获取最小数据差异的结果  \n具体为:\n\n>根据复合函数求导,对误差e求theta偏导,获取梯度Δ:\n![formula](机器学习-一-简单线性回归/formula3.png)\n> 根据梯度下降的公式:x <-- x - λ*Δ，每次迭代，重复梯度下降的过程.\n> 代码流程为:\n> 1. 设定初始的theta值，迭代次数iters，步长alpha\n> 2. 根据 梯度下降的公式不断修改 theta\n> 2. 根据迭代次数重复上一个步骤\n\n\n```python\ndef gradientDescent(X, Y, theta, alpha, inters):\n  \"\"\"\n  梯度下降方法\n  alpha: 步长\n  inters: 迭代次数\n  \"\"\"\n  temp = np.zeros(theta.shape) # 用于缓存theta\n  arguments = int(theta.ravel().shape[0]) # theta的元素数量\n  cost = np.zeros(len(X)) # 用于保存每次迭代后theta所对应的误差\n  for i in xrange(iters):\n    error = X*theta.T - Y\n    for j in xrange(arguments): # 修改参数的每一个元素\n      term = np.multiply(error, X[:,j])\n      temp[:,j] = theta[:,j] - (alpha/len(x)) * np.sum(term)\n    theta = temp\n    cost[i] = computeCast(X, Y, theta)\n    return theta, cost\n```\n## 4. 观察结果\n设定一些参数,运行\n```Python\ntheta = np.matrix(np.array([0,0]))\nalpha = 0.01\ninters = 1000\ng, cost = gradientDescent(X, Y,theta, alpha, iters)\nprint g # [[-3.24140214  1.1272942 ]]\nprint cost[-1] # 4.51595550308\n```\n\n根据得到的theta画出预测的直线\n```python\nx_array = np.linspace(data.Populcation.min(), data.Populocation.max(), 1000)\ny_array = theta[:,0] + theta[:,1] * x_array\nfig, ax = plt.subplots(figsize=(12,8))\n\nax.plot(x_array, y_array, 'r', label='Prediction') # 画出预测曲线，红色\nax.scatter(data.Populcation, data.Profit, label='traning data') # 画出训练数据\nax.legend(loc=2) # 提示，标注\nax.set_xlabel('Populocation') # 横坐标\nax.set_ylabel('Profit') # 纵坐标\nax.set_title('Predicted Profit vs. Population Size')\nplt.show()\n```\n如图:  \n![example2](机器学习-一-简单线性回归/figure2.png)\n\n## 5.查看迭代次数与误差的关系\n```python\n# 尝试画出 iters - cost 的曲线\nfig , ax = plt.subplots(figsize=(12,8))\nax.plot(np.arange(iters), cost, 'r')\nax.set_xlabel('iters')\nax.set_ylabel('cost')\nax.set_title('iters vs cost')\nplt.show()\n```\n如图:  \n![example3](机器学习-一-简单线性回归/figure3.png)\n","source":"_posts/机器学习-一-简单线性回归.md","raw":"---\ntitle: '机器学习(一):简单线性回归'\ndate: 2017-02-21 10:31:06\nauthor: \"月牙天冲\"\ntags:\n  - 机器学习\n  - 学习笔记\n---\n\n## 1. 获取数据\n假设你是一家连锁店的老板，希望在另一个城市开分店，现在有一组城市的人口与利润的关系图，\n通过这些数据预测不同人口的城市的利润\n数据可以通过这个连接[下载](ex1data1.txt)\n\n\n```python\n# 导入模块\nimport numpy as np\nimport pandas as pd\nimport matlib.pyplot as plt\n\n# 读取数据\ndata = pd.read_csv('ex1data1.txt', names=['Population','Profit'], header=None) #设置列名\n# 画出数据的散点图查看\ndata.plot(kind='scatter', x='Population', y='Profit', figsize=(12,8))\nplt.show()\n```\n\n如图:  \n![example1](机器学习-一-简单线性回归/figure1.png)\n\n## 2.梯度下降\n\n>梯度下降:\n>假设 y = f(x), 求y最小值时候的x.\n>则 梯度 Δ = f'(x)\n>设定好一定的步长 λ\n>x <-- x - λ*Δ\n>代理初始的x后每次迭代x都往梯度下降的方向挪动一定的步长(其中Δ控制方向)。\n>迭代多次后,当两次x带入后获取的y数值相差极小时,此时的x为最优解\n\n\n## 3.实现简单的线性回归\n只有一个因素影响数据, 假设直线 Y = theta0 + theta1*X1  \n转换为向量的思想，则:Y = theta * (1, X)\n![formula](机器学习-一-简单线性回归/formula4.png)\n\n误差计算方法:\n![formula](机器学习-一-简单线性回归/formula1.png)\n\n即:\n![formula](机器学习-一-简单线性回归/formula2.png)\n\n```Python\ndef computeCast(X, Y, theta):\n  \"\"\"\n  计算误差的方法\n  X: 输入的数据,格式：[[1,2],[1,3],[1,4]...]\n  Y: 实际结果, 格式：[[1],[2],[3]...]\n  theta: 参数, 向量，格式：[0,0]\n  \"\"\"\n  error = np.sqrt(np.power(((X*theta.T) - Y), 2))\n  return np.sum(error)/(len(X)*2) #TODO 为什么要多除一个2？\n\n# 例\ntheta  = np.matrix(np.array([0,0]))\ncost = computeCast(X, Y, theta)\nprint cost # 32.0727338775\n\n```\n\n\n\n使用梯度下降的方法调节参数theta,获取最小数据差异的结果  \n具体为:\n\n>根据复合函数求导,对误差e求theta偏导,获取梯度Δ:\n![formula](机器学习-一-简单线性回归/formula3.png)\n> 根据梯度下降的公式:x <-- x - λ*Δ，每次迭代，重复梯度下降的过程.\n> 代码流程为:\n> 1. 设定初始的theta值，迭代次数iters，步长alpha\n> 2. 根据 梯度下降的公式不断修改 theta\n> 2. 根据迭代次数重复上一个步骤\n\n\n```python\ndef gradientDescent(X, Y, theta, alpha, inters):\n  \"\"\"\n  梯度下降方法\n  alpha: 步长\n  inters: 迭代次数\n  \"\"\"\n  temp = np.zeros(theta.shape) # 用于缓存theta\n  arguments = int(theta.ravel().shape[0]) # theta的元素数量\n  cost = np.zeros(len(X)) # 用于保存每次迭代后theta所对应的误差\n  for i in xrange(iters):\n    error = X*theta.T - Y\n    for j in xrange(arguments): # 修改参数的每一个元素\n      term = np.multiply(error, X[:,j])\n      temp[:,j] = theta[:,j] - (alpha/len(x)) * np.sum(term)\n    theta = temp\n    cost[i] = computeCast(X, Y, theta)\n    return theta, cost\n```\n## 4. 观察结果\n设定一些参数,运行\n```Python\ntheta = np.matrix(np.array([0,0]))\nalpha = 0.01\ninters = 1000\ng, cost = gradientDescent(X, Y,theta, alpha, iters)\nprint g # [[-3.24140214  1.1272942 ]]\nprint cost[-1] # 4.51595550308\n```\n\n根据得到的theta画出预测的直线\n```python\nx_array = np.linspace(data.Populcation.min(), data.Populocation.max(), 1000)\ny_array = theta[:,0] + theta[:,1] * x_array\nfig, ax = plt.subplots(figsize=(12,8))\n\nax.plot(x_array, y_array, 'r', label='Prediction') # 画出预测曲线，红色\nax.scatter(data.Populcation, data.Profit, label='traning data') # 画出训练数据\nax.legend(loc=2) # 提示，标注\nax.set_xlabel('Populocation') # 横坐标\nax.set_ylabel('Profit') # 纵坐标\nax.set_title('Predicted Profit vs. Population Size')\nplt.show()\n```\n如图:  \n![example2](机器学习-一-简单线性回归/figure2.png)\n\n## 5.查看迭代次数与误差的关系\n```python\n# 尝试画出 iters - cost 的曲线\nfig , ax = plt.subplots(figsize=(12,8))\nax.plot(np.arange(iters), cost, 'r')\nax.set_xlabel('iters')\nax.set_ylabel('cost')\nax.set_title('iters vs cost')\nplt.show()\n```\n如图:  \n![example3](机器学习-一-简单线性回归/figure3.png)\n","slug":"机器学习-一-简单线性回归","published":1,"updated":"2017-02-22T06:07:00.124Z","_id":"cizf5vg8l00050tm8r3dihhel","comments":1,"layout":"post","photos":[],"link":"","content":"<h2 id=\"1-获取数据\"><a href=\"#1-获取数据\" class=\"headerlink\" title=\"1. 获取数据\"></a>1. 获取数据</h2><p>假设你是一家连锁店的老板，希望在另一个城市开分店，现在有一组城市的人口与利润的关系图，<br>通过这些数据预测不同人口的城市的利润<br>数据可以通过这个连接<a href=\"ex1data1.txt\">下载</a></p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># 导入模块</span></div><div class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</div><div class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</div><div class=\"line\"><span class=\"keyword\">import</span> matlib.pyplot <span class=\"keyword\">as</span> plt</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># 读取数据</span></div><div class=\"line\">data = pd.read_csv(<span class=\"string\">'ex1data1.txt'</span>, names=[<span class=\"string\">'Population'</span>,<span class=\"string\">'Profit'</span>], header=<span class=\"keyword\">None</span>) <span class=\"comment\">#设置列名</span></div><div class=\"line\"><span class=\"comment\"># 画出数据的散点图查看</span></div><div class=\"line\">data.plot(kind=<span class=\"string\">'scatter'</span>, x=<span class=\"string\">'Population'</span>, y=<span class=\"string\">'Profit'</span>, figsize=(<span class=\"number\">12</span>,<span class=\"number\">8</span>))</div><div class=\"line\">plt.show()</div></pre></td></tr></table></figure>\n<p>如图:<br><img src=\"/2017/02/21/机器学习-一-简单线性回归/figure1.png\" alt=\"example1\"></p>\n<h2 id=\"2-梯度下降\"><a href=\"#2-梯度下降\" class=\"headerlink\" title=\"2.梯度下降\"></a>2.梯度下降</h2><blockquote>\n<p>梯度下降:<br>假设 y = f(x), 求y最小值时候的x.<br>则 梯度 Δ = f’(x)<br>设定好一定的步长 λ<br>x &lt;– x - λ*Δ<br>代理初始的x后每次迭代x都往梯度下降的方向挪动一定的步长(其中Δ控制方向)。<br>迭代多次后,当两次x带入后获取的y数值相差极小时,此时的x为最优解</p>\n</blockquote>\n<h2 id=\"3-实现简单的线性回归\"><a href=\"#3-实现简单的线性回归\" class=\"headerlink\" title=\"3.实现简单的线性回归\"></a>3.实现简单的线性回归</h2><p>只有一个因素影响数据, 假设直线 Y = theta0 + theta1<em>X1<br>转换为向量的思想，则:Y = theta </em> (1, X)<br><img src=\"/2017/02/21/机器学习-一-简单线性回归/formula4.png\" alt=\"formula\"></p>\n<p>误差计算方法:<br><img src=\"/2017/02/21/机器学习-一-简单线性回归/formula1.png\" alt=\"formula\"></p>\n<p>即:<br><img src=\"/2017/02/21/机器学习-一-简单线性回归/formula2.png\" alt=\"formula\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">computeCast</span><span class=\"params\">(X, Y, theta)</span>:</span></div><div class=\"line\">  <span class=\"string\">\"\"\"</span></div><div class=\"line\">  计算误差的方法</div><div class=\"line\">  X: 输入的数据,格式：[[1,2],[1,3],[1,4]...]</div><div class=\"line\">  Y: 实际结果, 格式：[[1],[2],[3]...]</div><div class=\"line\">  theta: 参数, 向量，格式：[0,0]</div><div class=\"line\">  \"\"\"</div><div class=\"line\">  error = np.sqrt(np.power(((X*theta.T) - Y), <span class=\"number\">2</span>))</div><div class=\"line\">  <span class=\"keyword\">return</span> np.sum(error)/(len(X)*<span class=\"number\">2</span>) <span class=\"comment\">#TODO 为什么要多除一个2？</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># 例</span></div><div class=\"line\">theta  = np.matrix(np.array([<span class=\"number\">0</span>,<span class=\"number\">0</span>]))</div><div class=\"line\">cost = computeCast(X, Y, theta)</div><div class=\"line\"><span class=\"keyword\">print</span> cost <span class=\"comment\"># 32.0727338775</span></div></pre></td></tr></table></figure>\n<p>使用梯度下降的方法调节参数theta,获取最小数据差异的结果<br>具体为:</p>\n<blockquote>\n<p>根据复合函数求导,对误差e求theta偏导,获取梯度Δ:<br><img src=\"/2017/02/21/机器学习-一-简单线性回归/formula3.png\" alt=\"formula\"><br>根据梯度下降的公式:x &lt;– x - λ*Δ，每次迭代，重复梯度下降的过程.<br>代码流程为:</p>\n<ol>\n<li>设定初始的theta值，迭代次数iters，步长alpha</li>\n<li>根据 梯度下降的公式不断修改 theta</li>\n<li>根据迭代次数重复上一个步骤</li>\n</ol>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">gradientDescent</span><span class=\"params\">(X, Y, theta, alpha, inters)</span>:</span></div><div class=\"line\">  <span class=\"string\">\"\"\"</span></div><div class=\"line\">  梯度下降方法</div><div class=\"line\">  alpha: 步长</div><div class=\"line\">  inters: 迭代次数</div><div class=\"line\">  \"\"\"</div><div class=\"line\">  temp = np.zeros(theta.shape) <span class=\"comment\"># 用于缓存theta</span></div><div class=\"line\">  arguments = int(theta.ravel().shape[<span class=\"number\">0</span>]) <span class=\"comment\"># theta的元素数量</span></div><div class=\"line\">  cost = np.zeros(len(X)) <span class=\"comment\"># 用于保存每次迭代后theta所对应的误差</span></div><div class=\"line\">  <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> xrange(iters):</div><div class=\"line\">    error = X*theta.T - Y</div><div class=\"line\">    <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> xrange(arguments): <span class=\"comment\"># 修改参数的每一个元素</span></div><div class=\"line\">      term = np.multiply(error, X[:,j])</div><div class=\"line\">      temp[:,j] = theta[:,j] - (alpha/len(x)) * np.sum(term)</div><div class=\"line\">    theta = temp</div><div class=\"line\">    cost[i] = computeCast(X, Y, theta)</div><div class=\"line\">    <span class=\"keyword\">return</span> theta, cost</div></pre></td></tr></table></figure>\n<h2 id=\"4-观察结果\"><a href=\"#4-观察结果\" class=\"headerlink\" title=\"4. 观察结果\"></a>4. 观察结果</h2><p>设定一些参数,运行<br><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><div class=\"line\">theta = np.matrix(np.array([<span class=\"number\">0</span>,<span class=\"number\">0</span>]))</div><div class=\"line\">alpha = <span class=\"number\">0.01</span></div><div class=\"line\">inters = <span class=\"number\">1000</span></div><div class=\"line\">g, cost = gradientDescent(X, Y,theta, alpha, iters)</div><div class=\"line\"><span class=\"keyword\">print</span> g <span class=\"comment\"># [[-3.24140214  1.1272942 ]]</span></div><div class=\"line\"><span class=\"keyword\">print</span> cost[<span class=\"number\">-1</span>] <span class=\"comment\"># 4.51595550308</span></div></pre></td></tr></table></figure></p>\n<p>根据得到的theta画出预测的直线<br><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><div class=\"line\">x_array = np.linspace(data.Populcation.min(), data.Populocation.max(), <span class=\"number\">1000</span>)</div><div class=\"line\">y_array = theta[:,<span class=\"number\">0</span>] + theta[:,<span class=\"number\">1</span>] * x_array</div><div class=\"line\">fig, ax = plt.subplots(figsize=(<span class=\"number\">12</span>,<span class=\"number\">8</span>))</div><div class=\"line\"></div><div class=\"line\">ax.plot(x_array, y_array, <span class=\"string\">'r'</span>, label=<span class=\"string\">'Prediction'</span>) <span class=\"comment\"># 画出预测曲线，红色</span></div><div class=\"line\">ax.scatter(data.Populcation, data.Profit, label=<span class=\"string\">'traning data'</span>) <span class=\"comment\"># 画出训练数据</span></div><div class=\"line\">ax.legend(loc=<span class=\"number\">2</span>) <span class=\"comment\"># 提示，标注</span></div><div class=\"line\">ax.set_xlabel(<span class=\"string\">'Populocation'</span>) <span class=\"comment\"># 横坐标</span></div><div class=\"line\">ax.set_ylabel(<span class=\"string\">'Profit'</span>) <span class=\"comment\"># 纵坐标</span></div><div class=\"line\">ax.set_title(<span class=\"string\">'Predicted Profit vs. Population Size'</span>)</div><div class=\"line\">plt.show()</div></pre></td></tr></table></figure></p>\n<p>如图:<br><img src=\"/2017/02/21/机器学习-一-简单线性回归/figure2.png\" alt=\"example2\"></p>\n<h2 id=\"5-查看迭代次数与误差的关系\"><a href=\"#5-查看迭代次数与误差的关系\" class=\"headerlink\" title=\"5.查看迭代次数与误差的关系\"></a>5.查看迭代次数与误差的关系</h2><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># 尝试画出 iters - cost 的曲线</span></div><div class=\"line\">fig , ax = plt.subplots(figsize=(<span class=\"number\">12</span>,<span class=\"number\">8</span>))</div><div class=\"line\">ax.plot(np.arange(iters), cost, <span class=\"string\">'r'</span>)</div><div class=\"line\">ax.set_xlabel(<span class=\"string\">'iters'</span>)</div><div class=\"line\">ax.set_ylabel(<span class=\"string\">'cost'</span>)</div><div class=\"line\">ax.set_title(<span class=\"string\">'iters vs cost'</span>)</div><div class=\"line\">plt.show()</div></pre></td></tr></table></figure>\n<p>如图:<br><img src=\"/2017/02/21/机器学习-一-简单线性回归/figure3.png\" alt=\"example3\"></p>\n","excerpt":"","more":"<h2 id=\"1-获取数据\"><a href=\"#1-获取数据\" class=\"headerlink\" title=\"1. 获取数据\"></a>1. 获取数据</h2><p>假设你是一家连锁店的老板，希望在另一个城市开分店，现在有一组城市的人口与利润的关系图，<br>通过这些数据预测不同人口的城市的利润<br>数据可以通过这个连接<a href=\"ex1data1.txt\">下载</a></p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># 导入模块</span></div><div class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</div><div class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</div><div class=\"line\"><span class=\"keyword\">import</span> matlib.pyplot <span class=\"keyword\">as</span> plt</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># 读取数据</span></div><div class=\"line\">data = pd.read_csv(<span class=\"string\">'ex1data1.txt'</span>, names=[<span class=\"string\">'Population'</span>,<span class=\"string\">'Profit'</span>], header=<span class=\"keyword\">None</span>) <span class=\"comment\">#设置列名</span></div><div class=\"line\"><span class=\"comment\"># 画出数据的散点图查看</span></div><div class=\"line\">data.plot(kind=<span class=\"string\">'scatter'</span>, x=<span class=\"string\">'Population'</span>, y=<span class=\"string\">'Profit'</span>, figsize=(<span class=\"number\">12</span>,<span class=\"number\">8</span>))</div><div class=\"line\">plt.show()</div></pre></td></tr></table></figure>\n<p>如图:<br><img src=\"/2017/02/21/机器学习-一-简单线性回归/figure1.png\" alt=\"example1\"></p>\n<h2 id=\"2-梯度下降\"><a href=\"#2-梯度下降\" class=\"headerlink\" title=\"2.梯度下降\"></a>2.梯度下降</h2><blockquote>\n<p>梯度下降:<br>假设 y = f(x), 求y最小值时候的x.<br>则 梯度 Δ = f’(x)<br>设定好一定的步长 λ<br>x &lt;– x - λ*Δ<br>代理初始的x后每次迭代x都往梯度下降的方向挪动一定的步长(其中Δ控制方向)。<br>迭代多次后,当两次x带入后获取的y数值相差极小时,此时的x为最优解</p>\n</blockquote>\n<h2 id=\"3-实现简单的线性回归\"><a href=\"#3-实现简单的线性回归\" class=\"headerlink\" title=\"3.实现简单的线性回归\"></a>3.实现简单的线性回归</h2><p>只有一个因素影响数据, 假设直线 Y = theta0 + theta1<em>X1<br>转换为向量的思想，则:Y = theta </em> (1, X)<br><img src=\"/2017/02/21/机器学习-一-简单线性回归/formula4.png\" alt=\"formula\"></p>\n<p>误差计算方法:<br><img src=\"/2017/02/21/机器学习-一-简单线性回归/formula1.png\" alt=\"formula\"></p>\n<p>即:<br><img src=\"/2017/02/21/机器学习-一-简单线性回归/formula2.png\" alt=\"formula\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">computeCast</span><span class=\"params\">(X, Y, theta)</span>:</span></div><div class=\"line\">  <span class=\"string\">\"\"\"</span></div><div class=\"line\">  计算误差的方法</div><div class=\"line\">  X: 输入的数据,格式：[[1,2],[1,3],[1,4]...]</div><div class=\"line\">  Y: 实际结果, 格式：[[1],[2],[3]...]</div><div class=\"line\">  theta: 参数, 向量，格式：[0,0]</div><div class=\"line\">  \"\"\"</div><div class=\"line\">  error = np.sqrt(np.power(((X*theta.T) - Y), <span class=\"number\">2</span>))</div><div class=\"line\">  <span class=\"keyword\">return</span> np.sum(error)/(len(X)*<span class=\"number\">2</span>) <span class=\"comment\">#TODO 为什么要多除一个2？</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># 例</span></div><div class=\"line\">theta  = np.matrix(np.array([<span class=\"number\">0</span>,<span class=\"number\">0</span>]))</div><div class=\"line\">cost = computeCast(X, Y, theta)</div><div class=\"line\"><span class=\"keyword\">print</span> cost <span class=\"comment\"># 32.0727338775</span></div></pre></td></tr></table></figure>\n<p>使用梯度下降的方法调节参数theta,获取最小数据差异的结果<br>具体为:</p>\n<blockquote>\n<p>根据复合函数求导,对误差e求theta偏导,获取梯度Δ:<br><img src=\"/2017/02/21/机器学习-一-简单线性回归/formula3.png\" alt=\"formula\"><br>根据梯度下降的公式:x &lt;– x - λ*Δ，每次迭代，重复梯度下降的过程.<br>代码流程为:</p>\n<ol>\n<li>设定初始的theta值，迭代次数iters，步长alpha</li>\n<li>根据 梯度下降的公式不断修改 theta</li>\n<li>根据迭代次数重复上一个步骤</li>\n</ol>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">gradientDescent</span><span class=\"params\">(X, Y, theta, alpha, inters)</span>:</span></div><div class=\"line\">  <span class=\"string\">\"\"\"</span></div><div class=\"line\">  梯度下降方法</div><div class=\"line\">  alpha: 步长</div><div class=\"line\">  inters: 迭代次数</div><div class=\"line\">  \"\"\"</div><div class=\"line\">  temp = np.zeros(theta.shape) <span class=\"comment\"># 用于缓存theta</span></div><div class=\"line\">  arguments = int(theta.ravel().shape[<span class=\"number\">0</span>]) <span class=\"comment\"># theta的元素数量</span></div><div class=\"line\">  cost = np.zeros(len(X)) <span class=\"comment\"># 用于保存每次迭代后theta所对应的误差</span></div><div class=\"line\">  <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> xrange(iters):</div><div class=\"line\">    error = X*theta.T - Y</div><div class=\"line\">    <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> xrange(arguments): <span class=\"comment\"># 修改参数的每一个元素</span></div><div class=\"line\">      term = np.multiply(error, X[:,j])</div><div class=\"line\">      temp[:,j] = theta[:,j] - (alpha/len(x)) * np.sum(term)</div><div class=\"line\">    theta = temp</div><div class=\"line\">    cost[i] = computeCast(X, Y, theta)</div><div class=\"line\">    <span class=\"keyword\">return</span> theta, cost</div></pre></td></tr></table></figure>\n<h2 id=\"4-观察结果\"><a href=\"#4-观察结果\" class=\"headerlink\" title=\"4. 观察结果\"></a>4. 观察结果</h2><p>设定一些参数,运行<br><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><div class=\"line\">theta = np.matrix(np.array([<span class=\"number\">0</span>,<span class=\"number\">0</span>]))</div><div class=\"line\">alpha = <span class=\"number\">0.01</span></div><div class=\"line\">inters = <span class=\"number\">1000</span></div><div class=\"line\">g, cost = gradientDescent(X, Y,theta, alpha, iters)</div><div class=\"line\"><span class=\"keyword\">print</span> g <span class=\"comment\"># [[-3.24140214  1.1272942 ]]</span></div><div class=\"line\"><span class=\"keyword\">print</span> cost[<span class=\"number\">-1</span>] <span class=\"comment\"># 4.51595550308</span></div></pre></td></tr></table></figure></p>\n<p>根据得到的theta画出预测的直线<br><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><div class=\"line\">x_array = np.linspace(data.Populcation.min(), data.Populocation.max(), <span class=\"number\">1000</span>)</div><div class=\"line\">y_array = theta[:,<span class=\"number\">0</span>] + theta[:,<span class=\"number\">1</span>] * x_array</div><div class=\"line\">fig, ax = plt.subplots(figsize=(<span class=\"number\">12</span>,<span class=\"number\">8</span>))</div><div class=\"line\"></div><div class=\"line\">ax.plot(x_array, y_array, <span class=\"string\">'r'</span>, label=<span class=\"string\">'Prediction'</span>) <span class=\"comment\"># 画出预测曲线，红色</span></div><div class=\"line\">ax.scatter(data.Populcation, data.Profit, label=<span class=\"string\">'traning data'</span>) <span class=\"comment\"># 画出训练数据</span></div><div class=\"line\">ax.legend(loc=<span class=\"number\">2</span>) <span class=\"comment\"># 提示，标注</span></div><div class=\"line\">ax.set_xlabel(<span class=\"string\">'Populocation'</span>) <span class=\"comment\"># 横坐标</span></div><div class=\"line\">ax.set_ylabel(<span class=\"string\">'Profit'</span>) <span class=\"comment\"># 纵坐标</span></div><div class=\"line\">ax.set_title(<span class=\"string\">'Predicted Profit vs. Population Size'</span>)</div><div class=\"line\">plt.show()</div></pre></td></tr></table></figure></p>\n<p>如图:<br><img src=\"/2017/02/21/机器学习-一-简单线性回归/figure2.png\" alt=\"example2\"></p>\n<h2 id=\"5-查看迭代次数与误差的关系\"><a href=\"#5-查看迭代次数与误差的关系\" class=\"headerlink\" title=\"5.查看迭代次数与误差的关系\"></a>5.查看迭代次数与误差的关系</h2><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># 尝试画出 iters - cost 的曲线</span></div><div class=\"line\">fig , ax = plt.subplots(figsize=(<span class=\"number\">12</span>,<span class=\"number\">8</span>))</div><div class=\"line\">ax.plot(np.arange(iters), cost, <span class=\"string\">'r'</span>)</div><div class=\"line\">ax.set_xlabel(<span class=\"string\">'iters'</span>)</div><div class=\"line\">ax.set_ylabel(<span class=\"string\">'cost'</span>)</div><div class=\"line\">ax.set_title(<span class=\"string\">'iters vs cost'</span>)</div><div class=\"line\">plt.show()</div></pre></td></tr></table></figure>\n<p>如图:<br><img src=\"/2017/02/21/机器学习-一-简单线性回归/figure3.png\" alt=\"example3\"></p>\n"}],"PostAsset":[{"_id":"source/_posts/Dockerfile的创建/docker.jpg","slug":"docker.jpg","post":"cizf5vg8400000tm8t0vk05co","modified":0,"renderable":0},{"_id":"source/_posts/docker-本地Registry的部署/docker.jpg","slug":"docker.jpg","post":"cizf5vg8g00040tm8zihq1x8w","modified":0,"renderable":0},{"_id":"source/_posts/机器学习-一-简单线性回归/ex1data1.txt","slug":"ex1data1.txt","post":"cizf5vg8l00050tm8r3dihhel","modified":0,"renderable":0},{"_id":"source/_posts/机器学习-一-简单线性回归/figure1.png","slug":"figure1.png","post":"cizf5vg8l00050tm8r3dihhel","modified":0,"renderable":0},{"_id":"source/_posts/机器学习-一-简单线性回归/figure2.png","slug":"figure2.png","post":"cizf5vg8l00050tm8r3dihhel","modified":0,"renderable":0},{"_id":"source/_posts/机器学习-一-简单线性回归/figure3.png","slug":"figure3.png","post":"cizf5vg8l00050tm8r3dihhel","modified":0,"renderable":0},{"_id":"source/_posts/机器学习-一-简单线性回归/formula1.png","slug":"formula1.png","post":"cizf5vg8l00050tm8r3dihhel","modified":0,"renderable":0},{"_id":"source/_posts/机器学习-一-简单线性回归/formula2.png","slug":"formula2.png","post":"cizf5vg8l00050tm8r3dihhel","modified":0,"renderable":0},{"_id":"source/_posts/机器学习-一-简单线性回归/formula3.png","slug":"formula3.png","post":"cizf5vg8l00050tm8r3dihhel","modified":0,"renderable":0},{"_id":"source/_posts/机器学习-一-简单线性回归/formula4.png","slug":"formula4.png","post":"cizf5vg8l00050tm8r3dihhel","modified":0,"renderable":0}],"PostCategory":[],"PostTag":[{"post_id":"cizf5vg8400000tm8t0vk05co","tag_id":"cizf5vg8e00030tm8ndnqej0j","_id":"cizf5vg8n00070tm8mabkgr1l"},{"post_id":"cizf5vg8b00020tm8103bez5d","tag_id":"cizf5vg8n00060tm8nxm29rew","_id":"cizf5vg8p000a0tm86hyktok2"},{"post_id":"cizf5vg8b00020tm8103bez5d","tag_id":"cizf5vg8o00080tm8sjrf97i1","_id":"cizf5vg8q000b0tm8f83oynnq"},{"post_id":"cizf5vg8g00040tm8zihq1x8w","tag_id":"cizf5vg8e00030tm8ndnqej0j","_id":"cizf5vg8s000d0tm8uig30uh4"},{"post_id":"cizf5vg8l00050tm8r3dihhel","tag_id":"cizf5vg8q000c0tm81y38d1vb","_id":"cizgiin9d0008a2m8hojy3167"},{"post_id":"cizf5vg8l00050tm8r3dihhel","tag_id":"cizf5vg8v000e0tm8tp5j4zop","_id":"cizgiin9d0009a2m87dwfij4x"}],"Tag":[{"name":"docker","_id":"cizf5vg8e00030tm8ndnqej0j"},{"name":"Lover","_id":"cizf5vg8n00060tm8nxm29rew"},{"name":"智障","_id":"cizf5vg8o00080tm8sjrf97i1"},{"name":"机器学习","_id":"cizf5vg8q000c0tm81y38d1vb"},{"name":"学习笔记","_id":"cizf5vg8v000e0tm8tp5j4zop"}]}}